# Celery - Sistema de Tareas As√≠ncronas

## Arquitectura

El proyecto usa **un solo worker de Celery** que descubre autom√°ticamente todas las tasks de todos los m√≥dulos mediante el `service_locator`.

## C√≥mo Funciona

### 1. Registro de Tasks en M√≥dulos

Las tasks se definen como funciones Python normales (sin decorador `@app.task`):

```python
# modules/invoicing/adapter/input/tasks/invoice.py
import time

def emit_invoice():
    """Task para emitir factura"""
    time.sleep(10)
    return "Factura emitida!"
```

### 2. Registro en module.py

```python
# modules/invoicing/module.py
@property
def service(self) -> Dict[str, object]:
    from .adapter.input.tasks import invoice
    
    return {
        "purchase_invoice_service": self._container.purchase_invoice_service,
        # Registrar tasks
        "invoicing_tasks": {
            "emit_invoice": invoice.emit_invoice,
        },
    }
```

**Importante**: El nombre debe terminar en `_tasks` para que sea descubierto.

### 3. Descubrimiento Autom√°tico

```python
# core/celery/discovery.py
def create_celery_worker() -> Celery:
    app = Celery("hexa_worker", broker=env.RABBITMQ_URL, backend=env.REDIS_URL)
    
    # Buscar todos los servicios que terminan en "_tasks"
    task_services = {
        name: service
        for name, service in service_locator._services.items()
        if name.endswith("_tasks")
    }
    
    # Registrar cada funci√≥n como task
    for service_name, task_dict in task_services.items():
        module_name = service_name.replace("_tasks", "")
        for task_name, task_func in task_dict.items():
            full_task_name = f"{module_name}.{task_name}"
            app.task(name=full_task_name)(task_func)
    
    return app
```

### 4. Ejecuci√≥n del Worker

```python
# hexa/__main__.py
@cmd.command("celery-apps")
def run_celery():
    # Limpiar registros (para hot-reload)
    ModuleRegistry().clear()
    service_locator.clear()
    
    # Descubrir m√≥dulos
    discover_modules("modules", "module.py")
    
    # Crear worker con tasks
    app = create_celery_worker()
    app.worker_main(["worker", "--loglevel=INFO"])
```

## Usar Tasks

### Desde Python

```python
from celery import Celery

app = Celery("hexa_worker", broker=env.RABBITMQ_URL)

# Ejecutar task as√≠ncronamente
result = app.send_task("invoicing.emit_invoice")

# Esperar resultado
result.get(timeout=30)
```

### Desde un Endpoint

```python
@router.post("/emit")
async def emit_invoice():
    from celery import Celery
    app = Celery("hexa_worker", broker=env.RABBITMQ_URL)
    
    result = app.send_task("invoicing.emit_invoice")
    
    return {"task_id": result.id, "status": "pending"}
```

### Desde el Shell

```bash
# Dentro del container
docker compose -f compose.dev.yaml exec backend python

>>> from celery import Celery
>>> app = Celery("hexa_worker", broker="amqp://hexa:hexa@rabbit:5672/")
>>> result = app.send_task("invoicing.emit_invoice")
>>> result.get()
'Factura emitida!'
```

## Monitorear Tasks

### Ver logs del worker

```bash
docker compose -f compose.dev.yaml logs -f celery_worker
```

### Ver tasks registradas

Al iniciar el worker, ver√°s:

```
üì¶ Discovered 3 task services from service_locator
  ‚úì Registered: invoicing.emit_invoice
  ‚úì Registered: notifications.send_notification
  ‚úì Registered: yiqi_erp.emit_invoice

‚úÖ Total 3 tasks registered in Celery worker
```

### RabbitMQ Management

http://localhost:15672
- User: hexa
- Pass: hexa

Puedes ver:
- Queues activas
- Tasks pendientes
- Mensajes procesados

## Tasks con Par√°metros

```python
# modules/notifications/adapter/input/tasks/notification.py
def send_notification(user_id: int, message: str):
    """Enviar notificaci√≥n a usuario"""
    print(f"Sending to user {user_id}: {message}")
    return "Sent"

# Registrar
"notifications_tasks": {
    "send_notification": notification.send_notification,
}

# Usar
app.send_task("notifications.send_notification", args=[123, "Hello!"])
```

## Tasks con Retry

```python
def process_invoice():
    """Task con retry autom√°tico"""
    try:
        # L√≥gica que puede fallar
        result = external_api.call()
        return result
    except Exception as e:
        # Celery manejar√° el retry autom√°ticamente
        raise

# Para configurar retry, necesitas usar el decorador
# NOTA: Esto rompe el patr√≥n actual. Mejor manejar retry en el dominio
```

## Tasks Peri√≥dicas (Cron)

Para tasks peri√≥dicas, usar Celery Beat (no implementado a√∫n):

```python
# celery_beat_config.py
from celery.schedules import crontab

beat_schedule = {
    'sync-invoices-every-hour': {
        'task': 'invoicing.sync_invoices',
        'schedule': crontab(minute=0, hour='*/1'),
    },
}
```

## Hot Reload

El worker usa `watchfiles` para auto-reload:

```yaml
# compose.dev.yaml
celery_worker:
  command: /bin/sh -c "uv run watchfiles --filter python 'uv run hexa celery-apps' modules core shared"
```

Cuando cambias un archivo en `modules/`, `core/` o `shared/`, el worker se reinicia autom√°ticamente.

## Buenas Pr√°cticas

### ‚úÖ DO

- Funciones puras sin efectos secundarios inesperados
- Idempotentes (pueden ejecutarse m√∫ltiples veces sin problemas)
- Con timeout razonable
- Logging claro

### ‚ùå DON'T

- Tasks que modifican estado global
- Tasks que dependen de otras tasks s√≠ncronamente
- Tasks sin manejo de errores
- Tasks con l√≥gica de negocio compleja (usa use cases)

## Arquitectura Recomendada

```python
# ‚ùå MAL - L√≥gica en la task
def process_order():
    order = get_order()
    validate(order)
    calculate_total(order)
    save(order)

# ‚úÖ BIEN - Task delgada, use case hace el trabajo
def process_order(order_id: int):
    from modules.orders.application.service.order import OrderService
    from shared.interfaces.service_locator import service_locator
    
    service = service_locator.get_service("order_service")
    return await service.process_order(order_id)
```

## Troubleshooting

### Tasks no aparecen

1. Verificar que el servicio termine en `_tasks`
2. Ver logs del worker: `docker compose logs celery_worker`
3. Verificar que el m√≥dulo est√° registrado

### Worker no se conecta a RabbitMQ

Verificar `RABBITMQ_URL` en `.env`:
```
RABBITMQ_URL=amqp://hexa:hexa@rabbit:5672/
```

### Tasks fallan silenciosamente

Revisar logs del worker y Redis (result backend):
```bash
docker compose -f compose.dev.yaml logs celery_worker
docker compose -f compose.dev.yaml exec redis redis-cli -a <password> keys "*celery*"
```

## Pr√≥ximos Pasos

- [Service Locator](../architecture/04-service-locator.md) - C√≥mo llamar servicios desde tasks
- [Testing](../testing/05-service-tests.md) - C√≥mo testear tasks
